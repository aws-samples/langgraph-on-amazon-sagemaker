{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a380c04-4fc5-4789-b844-5a74cf1b02f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1>Using LangGraph with an Amazon SageMaker LLM Endpoint</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d169ffd1-286a-4374-93dd-d0a2eaa96b67",
   "metadata": {},
   "source": [
    "<h2>Overview</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c337db37-4f87-4d75-948b-b15571d7cb8c",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <a href=\"https://python.langchain.com/docs/langgraph\">LangGraph</a> is a library for building stateful, multi-actor applications with LLMs, built on top of (and intended to be used with) <a href=\"https://github.com/langchain-ai/langchain\">LangChain</a>. There are a number of <a href=\"https://github.com/langchain-ai/langgraph/tree/main/examples\">useful examples</a> for how to use it with the LangGraph documentation, and they are a great way to start out.\n",
    "</p>\n",
    "<p>\n",
    "    <a href=\"https://aws.amazon.com/pm/sagemaker/\">Amazon SageMaker</a> has great support for Large Language Models, and <b>its</b> <a href=\"https://github.com/aws-samples/amazon-sagemaker-generativeai/\">examples repository for generative AI</a> is again a great place for finding out how to use it.\n",
    "</p>\n",
    "<p>\n",
    "    This Notebook merges the two, demonstrating how a <b>simple</b> LangGraph Agent based on this example can be created to run with a LLM deployed in a SageMaker Endpoint. The model used could be replaced with any model deployed in SageMaker, but for this Notebook we are using <a href=\"https://mistral.ai/news/announcing-mistral-7b/\">Mistal 7B</a> due to its high performance and low footprint.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd36e41a-d6b3-4909-b99f-fff03e693c30",
   "metadata": {},
   "source": [
    "<h2>Setup / Prerequisites</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e32a323-d521-4ef9-9ddb-73e0657d08ab",
   "metadata": {},
   "source": [
    "<h3>LLM</h3>\n",
    "<p>\n",
    "    It is always a good idea to separate the <b>creation</b> of your LLM from the <b>use</b> of it. As this Notebook is about <b>using</b> Mistral 7B with LangGraph, the <b>creation</b> of the SageMaker Endpoint for Mistral 7B is not done here. For details on how to set up Mistral 7B Instruct using SageMaker Jumpstart, please refer to the <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html\">Developer Guide</a>, but it boils down to:\n",
    "<ol>\n",
    "    <li>Access Sagemaker Studio through the AWS Console</li>\n",
    "    <li>Inside Studio, navigate to \"Classic\"</li>\n",
    "    <li>Inside Studio Classic, navigate to Jumpstart</li>\n",
    "    <li>Search for \"Mistral 7B Instruct\"</li>\n",
    "    <li>Click \"Deploy\"</li>\n",
    "</ol>\n",
    "\n",
    "<h3>Tavily</h3>\n",
    "Just like the example in the LangGraph examples repository, this Notebook uses <a href=\"https://tavily.com/\">Tavily</a> for its web searching functionality. This is easy to do as the connections are already built into LangChain. In order to use Tavily, you need to have an API token. For small/temporary use like this, you can <a href=\"https://app.tavily.com/sign-in\">get a free one</a>. You will need to enter this into the Notebook in order for it to work. Please see the <b>README</b> in the main repository folder for details around using 3rd party services like Tavily.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3e155-8fa2-4c46-a17e-0e211e760381",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>Installing and updating to latest versions of SageMaker and LangChain/LangGraph</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbf8619f-3cf1-4e4b-a3a8-9649f04bc70a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip --root-user-action=ignore --quiet #One should always get latest of pip, just in case...\n",
    "%pip install sagemaker boto3 huggingface_hub --upgrade --root-user-action=ignore --quiet #These are used to interface with Amazon SageMaker\n",
    "%pip install -U langchain langgraph langchainhub --root-user-action=ignore --quiet #And this is the LangGraph bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa896e-7c26-4a46-8c61-cb96aede89b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>Some sample questions that we want answered</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868cfcd4-112d-48a3-92d4-6766137bb531",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p>\n",
    "Throughout this notebook, we'll make use of a set of questions that we will apply to \"test out\" our LangGraph setup. The idea is simple: We'll first try the questions against an unaided version of our LLM (maybe it's good enough to answer them just based on its own internal knowledge base that it built up using its training data?). We will then run the same questions against the setup where the model is managed by LangGraph and has access to tooling for searching the Web.\n",
    "</p>\n",
    "<p>\n",
    "    <b>DISCLAIMER!:</b> These questions are picked at random, based on being illustrative for this Notebook.    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d58b0f9e-0c05-4c65-9592-925d8c60eb7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "questions_and_answers = [\n",
    "    {\n",
    "        \"question\": \"What is the recipe of mayonnaise?\",\n",
    "        \"answers\": {}\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the name of the latest storm to hit the UK, and when and where did it cause the most damage?\",\n",
    "        \"answers\": {}\n",
    "    }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92da269-d056-4f2e-bce0-80519fb65cc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>Setting up and testing access to Mistral 7B Instruct model running as a SageMaker endpoint</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85d4f212-e99d-4dd9-b68e-439357c18e10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "SakeMaker Endpoint Name: jumpstart-dft-hf-llm-mistral-7b-instruct\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "#Reference to SageMaker Inference Endpoint in account that this notebook has access to. If you are running this in SageMaker Studio, then the access is implicit as long as the endpoint is in the same Domain and permissioned with defaults\n",
    "endpoint_name = input(\"SakeMaker Endpoint Name:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb1e545-df85-4b91-9f28-c1b76d19e30f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>Some helper functions for asking the LLM a simple question using plain boto3 APIs <b>without</b> using LangChain</h3>\n",
    "<p>\n",
    "    These are not required for the LangGraph implementation and are <b>very</b> boilerplate.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e09583-d585-47f7-b153-b9a4f5b32381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = boto3.client(\"runtime.sagemaker\")\n",
    "\n",
    "def query_endpoint(payload):    \n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=json.dumps(payload).encode(\"utf-8\")\n",
    "    )\n",
    "    response = response[\"Body\"].read().decode(\"utf8\")\n",
    "    response = json.loads(response)\n",
    "    return response\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def format_instructions(instructions: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format instructions where conversation roles must alternate user/assistant/user/assistant/...\"\"\"\n",
    "    prompt: List[str] = []\n",
    "    for user, answer in zip(instructions[::2], instructions[1::2]):\n",
    "        prompt.extend([\"<s>\", \"[INST] \", (user[\"content\"]).strip(), \" [/INST] \", (answer[\"content\"]).strip(), \"</s>\"])\n",
    "    prompt.extend([\"<s>\", \"[INST] \", (instructions[-1][\"content\"]).strip(), \" [/INST] \"])\n",
    "    return \"\".join(prompt)\n",
    "\n",
    "\n",
    "def print_instructions(prompt: str, response: str) -> None:\n",
    "    bold, unbold = '\\033[1m', '\\033[0m'\n",
    "    print(f\"{bold}> Input{unbold}\\n{prompt}\\n\\n{bold}> Output{unbold}\\n{response[0]['generated_text']}\\n\")\n",
    "    \n",
    "def ask_question(question):\n",
    "    instructions = [{\"role\": \"user\", \"content\": question}]\n",
    "\n",
    "    prompt = format_instructions(instructions)\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\"max_new_tokens\": 500, \"do_sample\": True, \"temperature\": 0.001}\n",
    "    }\n",
    "\n",
    "    response = query_endpoint(payload)\n",
    "    print_instructions(prompt, response)\n",
    "    return response[0]['generated_text']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a1fb36-9f6c-45b9-80d6-bf8ce337ce83",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>Asking our questions to the LLM without LangChain or LangGraph</h3>\n",
    "<p>\n",
    "    This is where we test out our SageMaker endpoint <b>without</b> using LangGraph, to give us that baseline.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eb58038-3035-42b5-b4df-571332d856f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m> Input\u001b[0m\n",
      "<s>[INST] What is the recipe of mayonnaise? [/INST] \n",
      "\n",
      "\u001b[1m> Output\u001b[0m\n",
      "1. Gather all ingredients: \n",
      "- 2 large egg yolks\n",
      "- 1 tablespoon Dijon mustard\n",
      "- 1 tablespoon white wine vinegar\n",
      "- 1 tablespoon lemon juice\n",
      "- 1/2 teaspoon salt\n",
      "- 1/4 teaspoon sugar\n",
      "- 1 cup vegetable oil\n",
      "- 1/4 cup olive oil\n",
      "- 1 tablespoon chopped fresh herbs (optional)\n",
      "\n",
      "2. In a medium bowl, whisk together egg yolks, mustard, vinegar, lemon juice, salt, and sugar until well combined.\n",
      "\n",
      "3. Slowly drizzle in the vegetable oil and olive oil, whisking constantly, until the mixture thickens and emulsifies.\n",
      "\n",
      "4. Taste and adjust seasoning as needed. Stir in chopped fresh herbs, if using.\n",
      "\n",
      "5. Cover the bowl with plastic wrap and refrigerate for at least 30 minutes before serving to allow the flavors to develop.\n",
      "\n",
      "6. Serve chilled and enjoy your homemade mayonnaise!\n",
      "\n",
      "\u001b[1m> Input\u001b[0m\n",
      "<s>[INST] What is the name of the latest storm to hit the UK, and when and where did it cause the most damage? [/INST] \n",
      "\n",
      "\u001b[1m> Output\u001b[0m\n",
      "1. Storm Arwen: This storm hit the UK on November 25th and 26th, 2021. It caused the most damage in Scotland, particularly in the north and northeast regions. The storm brought strong winds, heavy rainfall, and even snow in some areas. It caused widespread power outages, flooding, and damage to buildings and infrastructure.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question in questions_and_answers:\n",
    "    question[\"answers\"][\"llm\"] = ask_question(question[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ce0cc-971a-4061-a058-6a943cba9a3a",
   "metadata": {},
   "source": [
    "<h2>Setting up and Testing LangChain access to the SageMaker Endpoint</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a029cc4-17a6-46a3-bede-7abf2e3899ba",
   "metadata": {},
   "source": [
    "<h3>Hooking up the LLM to LangChain, and setting Content Handlers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0625e0a9-cefc-4ec1-921c-5f96cf004ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import SagemakerEndpoint\n",
    "from langchain_community.llms.sagemaker_endpoint import LLMContentHandler\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        #print(\"transforming_input\") #for debugging\n",
    "        #print(prompt) #for debugging\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        #print(\"transforming_output\") #for debugging\n",
    "        decoded_output = output.read().decode(\"utf-8\")\n",
    "        #print(\"decoded: \" +  decoded_output)\n",
    "        response_json = json.loads(decoded_output)\n",
    "        response = response_json[0][\"generated_text\"]\n",
    "        #print(\"response: \" + response) #for debugging\n",
    "        return response\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "llm=SagemakerEndpoint(\n",
    "        endpoint_name=endpoint_name,\n",
    "        region_name=\"us-east-1\",\n",
    "        model_kwargs={\"max_new_tokens\": 500, \"do_sample\": True, \"temperature\": 0.001}, #extending the max_tokens is VITAL, as the response will otherwise be cut, breaking the agent functionality by not giving it access to the LLM's full answer. The value has been picked empirically\n",
    "        content_handler=content_handler\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9210a640-6876-4bbc-b012-34b1c29a4adf",
   "metadata": {},
   "source": [
    "<h3>Asking our questions to the LLM with LangChain but still without LangGraph</h3>\n",
    "<p>\n",
    "    This is where we test out our Lang<b>Chain</b> setup, to see if it makes any difference against the baseline (it shouldn't, apart from the random differences that we get from the non deterministic nature of the LLM...).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2e876bb-940d-48fa-8859-395e17f3290b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the recipe of mayonnaise?\n",
      "Answer: \n",
      "\n",
      "Ingredients:\n",
      "- 2 large egg yolks\n",
      "- 1 tablespoon Dijon mustard\n",
      "- 2 tablespoons white wine vinegar\n",
      "- 1 tablespoon lemon juice\n",
      "- 1/2 teaspoon salt\n",
      "- 1/4 teaspoon sugar\n",
      "- 1 cup vegetable oil or canola oil\n",
      "\n",
      "Instructions:\n",
      "1. In a medium bowl, whisk together the egg yolks, mustard, vinegar, lemon juice, salt, and sugar until well combined.\n",
      "2. Slowly drizzle in the oil, whisking constantly, until the mixture thickens and emulsifies.\n",
      "3. Taste and adjust seasoning as needed.\n",
      "4. Cover the bowl with plastic wrap and refrigerate for at least 30 minutes before serving to allow the flavors to meld.\n",
      "5. Mayonnaise can be stored in an airtight container in the refrigerator for up to 7 days.\n",
      "Question: What is the name of the latest storm to hit the UK, and when and where did it cause the most damage?\n",
      "Answer: \n",
      "\n",
      "The latest storm to hit the UK is Storm Dennis, which hit the UK on February 14th and 15th, 2020. The storm caused the most damage in the South East of England, particularly in Kent and Sussex, where heavy rainfall and strong winds caused flooding, landslides, and damage to buildings and infrastructure.\n"
     ]
    }
   ],
   "source": [
    "for question in questions_and_answers:\n",
    "    print(\"Question: \" + question[\"question\"])\n",
    "    answer = llm.invoke(question[\"question\"])\n",
    "    print(\"Answer: \" + answer)\n",
    "    question[\"answers\"][\"langchain\"] = answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ac9c10-eff0-4afd-a840-b9e777b9e08a",
   "metadata": {},
   "source": [
    "<h2>Setting up LangGraph</h2>\n",
    "<p>\n",
    "    For this first example, we will replicate the basics of the high level \"Agent Executor\" example in the <a href=\"https://github.com/langchain-ai/langgraph/blob/main/examples/agent_executor/high-level.ipynb\">LangGraph github repo found here</a>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6436f72b-a888-4449-a912-2567de5d438c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tavily API Key: ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbed5a16-896f-4a96-bb2f-1675e950bacc",
   "metadata": {},
   "source": [
    "<h2>Create the LangChain agent and Define the Graph</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46d2fbde-72fe-498b-9ab9-474122a66104",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['agent_scratchpad', 'input', 'tools'] partial_variables={'chat_history': ''} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'chat_history', 'input', 'tools'], template=\"You are a helpful assistant. Help the user answer any questions.\\n\\nYou have access to the following tools:\\n\\n{tools}\\n\\nIn order to use a tool, you can use <tool></tool> and <tool_input></tool_input> tags. You will then get back a response in the form <observation></observation>\\nFor example, if you have a tool called 'search' that could run a google search, in order to search for the weather in SF you would respond:\\n\\n<tool>search</tool><tool_input>weather in SF</tool_input>\\n<observation>64 degrees</observation>\\n\\nWhen you are done, respond with a final answer between <final_answer></final_answer>. For example:\\n\\n<final_answer>The weather in SF is 64 degrees</final_answer>\\n\\nBegin!\\n\\nPrevious Conversation:\\n{chat_history}\\n\\nQuestion: {input}\\n{agent_scratchpad}\"))]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_xml_agent\n",
    "from langgraph.prebuilt import create_agent_executor\n",
    "\n",
    "tools = [TavilySearchResults(max_results=1)]\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/xml-agent-convo\")\n",
    "print(prompt)\n",
    "\n",
    "# Construct the XML agent using the LLM that we defined earlier\n",
    "agent_runnable  = create_xml_agent(llm, tools, prompt)\n",
    "app = create_agent_executor(agent_runnable, tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e45e93f-bcde-4a32-9a55-61863d34727e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>Ask the Agent our questions, and trace the calls through the Graph</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67bc38b0-5896-4755-9f82-d02293b2d0b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking question: What is the recipe of mayonnaise?\n",
      "\n",
      "Step 1:\n",
      "{'agent_outcome': AgentAction(tool='tavily_search_results_json', tool_input='recipe of mayonnaise', log='\\n<tool>tavily_search_results_json</tool><tool_input>recipe of mayonnaise')}\n",
      "----\n",
      "Step 2:\n",
      "{'intermediate_steps': [(AgentAction(tool='tavily_search_results_json', tool_input='recipe of mayonnaise', log='\\n<tool>tavily_search_results_json</tool><tool_input>recipe of mayonnaise'), '[{\\'url\\': \\'https://downshiftology.com/recipes/how-to-make-homemade-mayonnaise/\\', \\'content\\': \"Homemade Mayonnaise Ingredients  Homemade Mayonnaise Tips  Tasty Recipes That Use Mayonnaise  How To Fix Broken MayonnaiseAug 6, 2023 — Aug 6, 2023Learn how to make mayonnaise at home with this easy, foolproof recipe! It\\'s so fresh, creamy, and better than store-bought.\"}]')]}\n",
      "----\n",
      "Step 3:\n",
      "{'agent_outcome': AgentFinish(return_values={'output': 'Here is a recipe for homemade mayonnaise: <a href=\"https://downshiftology.com/recipes/how-to-make-homemade-mayonnaise/\">https://downshiftology.com/recipes/how-to-make-homemade-mayonnaise/</a>'}, log='\\n<final_answer>Here is a recipe for homemade mayonnaise: <a href=\"https://downshiftology.com/recipes/how-to-make-homemade-mayonnaise/\">https://downshiftology.com/recipes/how-to-make-homemade-mayonnaise/</a></final_answer>')}\n",
      "----\n",
      "Step 4:\n",
      "{'input': 'What is the recipe of mayonnaise?', 'chat_history': [], 'agent_outcome': AgentFinish(return_values={'output': 'Here is a recipe for homemade mayonnaise: <a href=\"https://downshiftology.com/recipes/how-to-make-homemade-mayonnaise/\">https://downshiftology.com/recipes/how-to-make-homemade-mayonnaise/</a>'}, log='\\n<final_answer>Here is a recipe for homemade mayonnaise: <a href=\"https://downshiftology.com/recipes/how-to-make-homemade-mayonnaise/\">https://downshiftology.com/recipes/how-to-make-homemade-mayonnaise/</a></final_answer>'), 'intermediate_steps': [(AgentAction(tool='tavily_search_results_json', tool_input='recipe of mayonnaise', log='\\n<tool>tavily_search_results_json</tool><tool_input>recipe of mayonnaise'), '[{\\'url\\': \\'https://downshiftology.com/recipes/how-to-make-homemade-mayonnaise/\\', \\'content\\': \"Homemade Mayonnaise Ingredients  Homemade Mayonnaise Tips  Tasty Recipes That Use Mayonnaise  How To Fix Broken MayonnaiseAug 6, 2023 — Aug 6, 2023Learn how to make mayonnaise at home with this easy, foolproof recipe! It\\'s so fresh, creamy, and better than store-bought.\"}]')]}\n",
      "----\n",
      "Final Outcome:\n",
      "\n",
      "return_values={'output': 'Here is a recipe for homemade mayonnaise: <a href=\"https://downshiftology.com/recipes/how-to-make-homemade-mayonnaise/\">https://downshiftology.com/recipes/how-to-make-homemade-mayonnaise/</a>'} log='\\n<final_answer>Here is a recipe for homemade mayonnaise: <a href=\"https://downshiftology.com/recipes/how-to-make-homemade-mayonnaise/\">https://downshiftology.com/recipes/how-to-make-homemade-mayonnaise/</a></final_answer>'\n",
      "----\n",
      "\n",
      "Asking question: What is the name of the latest storm to hit the UK, and when and where did it cause the most damage?\n",
      "\n",
      "Step 1:\n",
      "{'agent_outcome': AgentAction(tool='tavily_search_results_json', tool_input='latest storm in UK', log='\\n<tool>tavily_search_results_json</tool><tool_input>latest storm in UK')}\n",
      "----\n",
      "Step 2:\n",
      "{'intermediate_steps': [(AgentAction(tool='tavily_search_results_json', tool_input='latest storm in UK', log='\\n<tool>tavily_search_results_json</tool><tool_input>latest storm in UK'), \"[{'url': 'https://www.theguardian.com/uk-news/2024/jan/02/met-office-uk-weather-warning-storm-henk-britain', 'content': 'meteorologist in the south-west of England, David Braine, said this was the strongest there since the Burns Day storm  #StormHenk has been named and is forecast to bring very strong winds and heavy rain to parts of southern Britain  parts of southern Britain, with one area experiencing its strongest wind speeds for more than 30 years.  UK\\\\xa0weather: high winds to batter coasts of Wales and southern EnglandJan 2, 2024 — Jan 2, 2024Roads, bridges and rail lines were blocked as the first named storm of 2024 – Henk – swept across parts of southern Britain, with one area\\\\xa0...'}]\")]}\n",
      "----\n",
      "Step 3:\n",
      "{'agent_outcome': AgentFinish(return_values={'output': 'The name of the latest storm to hit the UK is Storm Henk, and it caused the most damage in the south-west of England.'}, log='\\n<final_answer>The name of the latest storm to hit the UK is Storm Henk, and it caused the most damage in the south-west of England.</final_answer>')}\n",
      "----\n",
      "Step 4:\n",
      "{'input': 'What is the name of the latest storm to hit the UK, and when and where did it cause the most damage?', 'chat_history': [], 'agent_outcome': AgentFinish(return_values={'output': 'The name of the latest storm to hit the UK is Storm Henk, and it caused the most damage in the south-west of England.'}, log='\\n<final_answer>The name of the latest storm to hit the UK is Storm Henk, and it caused the most damage in the south-west of England.</final_answer>'), 'intermediate_steps': [(AgentAction(tool='tavily_search_results_json', tool_input='latest storm in UK', log='\\n<tool>tavily_search_results_json</tool><tool_input>latest storm in UK'), \"[{'url': 'https://www.theguardian.com/uk-news/2024/jan/02/met-office-uk-weather-warning-storm-henk-britain', 'content': 'meteorologist in the south-west of England, David Braine, said this was the strongest there since the Burns Day storm  #StormHenk has been named and is forecast to bring very strong winds and heavy rain to parts of southern Britain  parts of southern Britain, with one area experiencing its strongest wind speeds for more than 30 years.  UK\\\\xa0weather: high winds to batter coasts of Wales and southern EnglandJan 2, 2024 — Jan 2, 2024Roads, bridges and rail lines were blocked as the first named storm of 2024 – Henk – swept across parts of southern Britain, with one area\\\\xa0...'}]\")]}\n",
      "----\n",
      "Final Outcome:\n",
      "\n",
      "return_values={'output': 'The name of the latest storm to hit the UK is Storm Henk, and it caused the most damage in the south-west of England.'} log='\\n<final_answer>The name of the latest storm to hit the UK is Storm Henk, and it caused the most damage in the south-west of England.</final_answer>'\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.agents import AgentFinish\n",
    "\n",
    "def ask_langgraph_question(question):\n",
    "    inputs = {\"input\": question, \"chat_history\": []}\n",
    "    print(\"Asking question: \" + question + \"\\n\")\n",
    "    \n",
    "    stepIndex = 1\n",
    "    agentFinish = None \n",
    "    for s in app.stream(inputs):\n",
    "        print(\"Step \" + str(stepIndex) + \":\")\n",
    "        agentValues = list(s.values())[0]\n",
    "        print(agentValues)\n",
    "        print(\"----\")\n",
    "        stepIndex = stepIndex + 1\n",
    "        if 'agent_outcome' in agentValues and isinstance(agentValues['agent_outcome'], AgentFinish):\n",
    "            agentFinish = agentValues['agent_outcome']\n",
    "\n",
    "\n",
    "    print(\"Final Outcome:\\n\")\n",
    "    print(agentFinish)\n",
    "    print(\"----\\n\")\n",
    "    return agentFinish.return_values[\"output\"]\n",
    "    \n",
    "    \n",
    "for question in questions_and_answers:\n",
    "    question[\"answers\"][\"langgraph\"] = ask_langgraph_question(question[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0bd02-a497-4d08-aa5e-aef87bc3599c",
   "metadata": {},
   "source": [
    "<h2>Compare our Approaches</h2>\n",
    "<p>\n",
    "We've done all the work! \n",
    "</p>\n",
    "<p>\n",
    "Now we just need to collect it all in a way where we can compare them, and hopefully see the benefits of using LangGraph over simply using the LLM's own knowledge base!\n",
    "</p>\n",
    "<p>\n",
    "As before, the LLM is non deterministic, so your answers may not match mine, but for me, LangGraph:\n",
    "    <ul>\n",
    "        <li>gave me a link to a recipe for mayo instead of trying to list it</li>\n",
    "        <li>correctly identified that the LLM's data model for the \"latest\" storms was outdated instead of giving an incorrect and outdated answer</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0bd488a-368f-4346-85ec-7e20726fafd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Question</td>\n",
       "      <td>LLM Only</td>\n",
       "      <td>LangChain</td>\n",
       "      <td>LangGraph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the recipe of mayonnaise?</td>\n",
       "      <td>1. Gather all ingredients: <br>- 2 large egg yolks<br>- 1 tablespoon Dijon mustard<br>- 1 tablespoon white wine vinegar<br>- 1 tablespoon lemon juice<br>- 1/2 teaspoon salt<br>- 1/4 teaspoon sugar<br>- 1 cup vegetable oil<br>- 1/4 cup olive oil<br>- 1 tablespoon chopped fresh herbs (optional)<br><br>2. In a medium bowl, whisk together egg yolks, mustard, vinegar, lemon juice, salt, and sugar until well combined.<br><br>3. Slowly drizzle in the vegetable oil and olive oil, whisking constantly, until the mixture thickens and emulsifies.<br><br>4. Taste and adjust seasoning as needed. Stir in chopped fresh herbs, if using.<br><br>5. Cover the bowl with plastic wrap and refrigerate for at least 30 minutes before serving to allow the flavors to develop.<br><br>6. Serve chilled and enjoy your homemade mayonnaise!</td>\n",
       "      <td><br><br>Ingredients:<br>- 2 large egg yolks<br>- 1 tablespoon Dijon mustard<br>- 2 tablespoons white wine vinegar<br>- 1 tablespoon lemon juice<br>- 1/2 teaspoon salt<br>- 1/4 teaspoon sugar<br>- 1 cup vegetable oil or canola oil<br><br>Instructions:<br>1. In a medium bowl, whisk together the egg yolks, mustard, vinegar, lemon juice, salt, and sugar until well combined.<br>2. Slowly drizzle in the oil, whisking constantly, until the mixture thickens and emulsifies.<br>3. Taste and adjust seasoning as needed.<br>4. Cover the bowl with plastic wrap and refrigerate for at least 30 minutes before serving to allow the flavors to meld.<br>5. Mayonnaise can be stored in an airtight container in the refrigerator for up to 7 days.</td>\n",
       "      <td>Here is a recipe for homemade mayonnaise: &lt;a href=\"https://downshiftology.com/recipes/how-to-make-homemade-mayonnaise/\"&gt;https://downshiftology.com/recipes/how-to-make-homemade-mayonnaise/&lt;/a&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the name of the latest storm to hit the UK, and when and where did it cause the most damage?</td>\n",
       "      <td>1. Storm Arwen: This storm hit the UK on November 25th and 26th, 2021. It caused the most damage in Scotland, particularly in the north and northeast regions. The storm brought strong winds, heavy rainfall, and even snow in some areas. It caused widespread power outages, flooding, and damage to buildings and infrastructure.</td>\n",
       "      <td><br><br>The latest storm to hit the UK is Storm Dennis, which hit the UK on February 14th and 15th, 2020. The storm caused the most damage in the South East of England, particularly in Kent and Sussex, where heavy rainfall and strong winds caused flooding, landslides, and damage to buildings and infrastructure.</td>\n",
       "      <td>The name of the latest storm to hit the UK is Storm Henk, and it caused the most damage in the south-west of England.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from tabulate import tabulate\n",
    "\n",
    "display_table = [[\"Question\", \"LLM Only\", \"LangChain\", \"LangGraph\"]]\n",
    "for qa in questions_and_answers:\n",
    "    display_table.append([qa[\"question\"], qa[\"answers\"][\"llm\"], qa[\"answers\"][\"langchain\"], qa[\"answers\"][\"langgraph\"]])\n",
    "    \n",
    "df = pd.DataFrame(display_table)\n",
    "df.style.set_properties(**{'text-align': 'left'}).set_table_styles([ dict(selector='th', props=[('text-align', 'left')] ) ])\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "display(HTML(df.to_html().replace(\"\\\\n\",\"<br>\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce753f0-626a-4ba6-beed-d9168cc4d89d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
